{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a802e5-a570-4371-8b57-296e307a73ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b334d7-44b2-4879-860c-a787bc3736db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68f11c4-51a7-4e47-a40e-eb5fc9e3c934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install torch\n",
    "# ! pip install peft\n",
    "# ! pip install tiktoken matplotlib pillow\n",
    "# ! pip install einops transformers_stream_generator\n",
    "# ! pip install torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cff9471",
   "metadata": {},
   "source": [
    "## Uncomment the following cell inorder to download the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f2a190-6203-4068-b6a0-86748105cd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.cloud import storage\n",
    "# import os\n",
    "\n",
    "# # Initialize a client using your service account key\n",
    "# client = storage.Client.from_service_account_json('/home/ubuntu/keen-snow-373818-93f44c794bab.json')\n",
    "\n",
    "# # Define your bucket name and folder to download\n",
    "# bucket_name = 'carla_dataset_bucket'\n",
    "# folder_prefix = 'output_carla_25K_automated/'  # Ensure the folder name ends with a '/'\n",
    "# local_download_path = '/home/ubuntu/output_carla_25K_automated'\n",
    "\n",
    "# # Folders to ignore\n",
    "# ignore_folders = ['checkpoint-1000/', 'checkpoint-2000/', 'checkpoint-3000/']\n",
    "\n",
    "# bucket = client.get_bucket(bucket_name)\n",
    "\n",
    "# # List blobs (files) in the folder\n",
    "# blobs = bucket.list_blobs(prefix=folder_prefix)\n",
    "\n",
    "# # Download each file in the folder\n",
    "# for blob in blobs:\n",
    "#     # Skip blobs in the ignore folders\n",
    "#     if any(blob.name.startswith(folder_prefix + ignore_folder) for ignore_folder in ignore_folders):\n",
    "#         continue\n",
    "\n",
    "#     # Construct the local file path\n",
    "#     local_file_path = os.path.join(local_download_path, blob.name[len(folder_prefix):])\n",
    "\n",
    "#     # Create directories if they don't exist\n",
    "#     os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n",
    "\n",
    "#     # Download the file\n",
    "#     blob.download_to_filename(local_file_path)\n",
    "#     print(f\"Downloaded {blob.name} to {local_file_path}\")\n",
    "\n",
    "# print(\"Download completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5298d347",
   "metadata": {},
   "source": [
    "## Run the following cells for testing the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e497fe5-d78a-41eb-8d10-624b64e0d12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# This will print the full path to the Python interpreter executable\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91781dd-ed0f-4267-b330-c98c0673375b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation import GenerationConfig\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# # Path to the local directory where the model files are stored\n",
    "local_model_path = '/home/ubuntu/output_carla_25K_automated'\n",
    "\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-VL-Chat\", trust_remote_code=True)\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    local_model_path, # path to the output directory\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0664dd4a-5e2b-42b6-9203-3ce600251663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "import time\n",
    "import json\n",
    "import ast\n",
    "\n",
    "def create_socket():\n",
    "    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    return s\n",
    "\n",
    "def listen_for_initiator(host, port):\n",
    "    sock = create_socket()\n",
    "    sock.bind((host, port))\n",
    "    sock.listen(1)\n",
    "    conn, addr = sock.accept()\n",
    "    return conn\n",
    "\n",
    "def connect_to_initiator(host, port):\n",
    "    sock = create_socket()\n",
    "    while True:\n",
    "        try:\n",
    "            sock.connect((host, port))\n",
    "            return sock\n",
    "        except ConnectionRefusedError:\n",
    "            time.sleep(1)  # Wait a bit before trying to reconnect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7ecfea-691c-4746-826c-27a66d2c5589",
   "metadata": {},
   "outputs": [],
   "source": [
    "host = '127.0.0.1'\n",
    "port = 12345\n",
    "\n",
    "mapping = {1:\"GO LEFT\" , 2: \"GO RIGHT\" ,  3: \"GO STRAIGHT\" , 4: \"FOLLOW THE CURRENT LANE\" , 5: \"CHANGE TO LANE LEFT\" , 6: \"CHANGE TO LANE RIGHT\" }\n",
    "\n",
    "print(\"Responder listening for Initiator...\")\n",
    "responder_socket = listen_for_initiator(host, port)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "\n",
    "    data = responder_socket.recv(1024)\n",
    "\n",
    "    SEND_DATA = json.loads(data.decode())\n",
    "    print(SEND_DATA)\n",
    "\n",
    "    image_path = \"/home/ubuntu/Carla_server/Carla_Repo/VisionVLM/\"+ SEND_DATA[\"rgb\"] # TODO: change this path to the root folder of the cloned DriveVLM repo + SEND_DATA[\"rgb\"]\n",
    "    #TODO: Change the following query based on the model you are using\n",
    "    query = f'Picture 1: <img>{image_path}</img>\\\\n Given this Ego centric image, \\\n",
    "            now you are autonomous driving agent, now your current position(x,y) is at \\\n",
    "            ({round(SEND_DATA[\"x\"],3)},{round(SEND_DATA[\"y\"],3)}) \\\\n \\\n",
    "            Your current speed is {round(SEND_DATA[\"speed\"],3)} m/s \\\\n \\\n",
    "            Your current angle is {round(SEND_DATA[\"theta\"],3)}  \\\\n \\\n",
    "            Your immediate command to follow is {mapping[SEND_DATA[\"command_near\"]]} and immediate target position is at \\\n",
    "            ({round(SEND_DATA[\"x_command_near\"],3)},{round(SEND_DATA[\"y_command_near\"],3)}) \\\\n \\\n",
    "            Your later target command to follow is {mapping[SEND_DATA[\"target_command\"]]} and later target position is at \\\n",
    "            ({round(SEND_DATA[\"x_target\"],3)},{round(SEND_DATA[\"y_target\"],3)}) \\\\n \\\n",
    "            The current command for applying brakes is {SEND_DATA[\"should_brake\"]} \\\\n \\\n",
    "            Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\\\n in the json format \\\n",
    "            For example: the output can be this json {{ \"throttle\" :x, \"steer\" : y, \"brake\": z }}'\n",
    "    print(query)\n",
    "    response, history = model.chat(tokenizer, query=query, history=None)\n",
    "    print(response)\n",
    "\n",
    "    response = ast.literal_eval(response)\n",
    "    response = json.dumps(response)\n",
    "\n",
    "    responder_socket.sendall(response.encode())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
