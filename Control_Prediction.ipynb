{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67a802e5-a570-4371-8b57-296e307a73ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4b334d7-44b2-4879-860c-a787bc3736db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a68f11c4-51a7-4e47-a40e-eb5fc9e3c934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install torch\n",
    "# ! pip install peft\n",
    "# ! pip install tiktoken matplotlib pillow\n",
    "# ! pip install einops transformers_stream_generator\n",
    "# ! pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72f2a190-6203-4068-b6a0-86748105cd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded output_carla_25K_automated/README.md to /home/ubuntu/output_carla_25K_automated/README.md\n",
      "Downloaded output_carla_25K_automated/adapter_config.json to /home/ubuntu/output_carla_25K_automated/adapter_config.json\n",
      "Downloaded output_carla_25K_automated/adapter_model.bin to /home/ubuntu/output_carla_25K_automated/adapter_model.bin\n",
      "Downloaded output_carla_25K_automated/qwen.tiktoken to /home/ubuntu/output_carla_25K_automated/qwen.tiktoken\n",
      "Downloaded output_carla_25K_automated/special_tokens_map.json to /home/ubuntu/output_carla_25K_automated/special_tokens_map.json\n",
      "Downloaded output_carla_25K_automated/tokenizer_config.json to /home/ubuntu/output_carla_25K_automated/tokenizer_config.json\n",
      "Downloaded output_carla_25K_automated/trainer_state.json to /home/ubuntu/output_carla_25K_automated/trainer_state.json\n",
      "Downloaded output_carla_25K_automated/training_args.bin to /home/ubuntu/output_carla_25K_automated/training_args.bin\n",
      "Download completed.\n"
     ]
    }
   ],
   "source": [
    "# from google.cloud import storage\n",
    "# import os\n",
    "\n",
    "# # Initialize a client using your service account key\n",
    "# client = storage.Client.from_service_account_json('/home/ubuntu/keen-snow-373818-93f44c794bab.json')\n",
    "\n",
    "# # Define your bucket name and folder to download\n",
    "# bucket_name = 'carla_dataset_bucket'\n",
    "# folder_prefix = 'output_carla_25K_automated/'  # Ensure the folder name ends with a '/'\n",
    "# local_download_path = '/home/ubuntu/output_carla_25K_automated'\n",
    "\n",
    "# # Folders to ignore\n",
    "# ignore_folders = ['checkpoint-1000/', 'checkpoint-2000/', 'checkpoint-3000/']\n",
    "\n",
    "# bucket = client.get_bucket(bucket_name)\n",
    "\n",
    "# # List blobs (files) in the folder\n",
    "# blobs = bucket.list_blobs(prefix=folder_prefix)\n",
    "\n",
    "# # Download each file in the folder\n",
    "# for blob in blobs:\n",
    "#     # Skip blobs in the ignore folders\n",
    "#     if any(blob.name.startswith(folder_prefix + ignore_folder) for ignore_folder in ignore_folders):\n",
    "#         continue\n",
    "\n",
    "#     # Construct the local file path\n",
    "#     local_file_path = os.path.join(local_download_path, blob.name[len(folder_prefix):])\n",
    "\n",
    "#     # Create directories if they don't exist\n",
    "#     os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n",
    "\n",
    "#     # Download the file\n",
    "#     blob.download_to_filename(local_file_path)\n",
    "#     print(f\"Downloaded {blob.name} to {local_file_path}\")\n",
    "\n",
    "# print(\"Download completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e497fe5-d78a-41eb-8d10-624b64e0d12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Carla_server/modelenv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# This will print the full path to the Python interpreter executable\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b91781dd-ed0f-4267-b330-c98c0673375b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to \"AutoModelForCausalLM.from_pretrained\".\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa06682068074873b4417985bc8c8689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation import GenerationConfig\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# # Path to the local directory where the model files are stored\n",
    "local_model_path = '/home/ubuntu/output_carla_25K_automated'\n",
    "\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-VL-Chat\", trust_remote_code=True)\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    local_model_path, # path to the output directory\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43491525-1cc7-4ec7-aa99-9c018895fe19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_11_16_00_21/input_image/0001.png</img>\\\\n Given this Ego centric image, now you are autonomous driving agent, now your current position(x,y) is at (122.055,-120.844) \\\\n Your current speed is 1.268 m/s \\\\n Your current angle is 6.279 \\\\n Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at (127.398,-120.813) \\\\n Your later target command to follow is GO RIGHT and later target position is at (127.398,-120.813) \\\\n The current command for applying brakes is False \\\\n Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\\\n in the json format For example: the output can be this json { \\\"throttle\\\" :x, \\\"steer\\\" : y, \\\"brake\\\": z }\"\n",
    "# response, history = model.chat(tokenizer, query=query, history=None)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0664dd4a-5e2b-42b6-9203-3ce600251663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "import time\n",
    "import json\n",
    "import ast\n",
    "\n",
    "def create_socket():\n",
    "    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    return s\n",
    "\n",
    "def listen_for_initiator(host, port):\n",
    "    sock = create_socket()\n",
    "    sock.bind((host, port))\n",
    "    sock.listen(1)\n",
    "    conn, addr = sock.accept()\n",
    "    return conn\n",
    "\n",
    "def connect_to_initiator(host, port):\n",
    "    sock = create_socket()\n",
    "    while True:\n",
    "        try:\n",
    "            sock.connect((host, port))\n",
    "            return sock\n",
    "        except ConnectionRefusedError:\n",
    "            time.sleep(1)  # Wait a bit before trying to reconnect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb7ecfea-691c-4746-826c-27a66d2c5589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responder listening for Initiator...\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0003.png', 'x': -0.001610136389302852, 'y': -3.104682434048109e-05, 'theta': 6.282840251922607, 'speed': 0.0, 'x_command_near': -175.24763159398083, 'y_command_near': -3.4570198106200576, 'command_near': 4, 'should_brake': True, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0003.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.002,-0.0) \\n             Your current speed is 0.0 m/s \\n             Your current angle is 6.283  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-175.248,-3.457) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.017, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0006.png', 'x': -0.001610136389302852, 'y': -3.104682434048109e-05, 'theta': 6.282840251922607, 'speed': -1.830156146526064e-08, 'x_command_near': -175.24763159398083, 'y_command_near': -3.4570198106200576, 'command_near': 4, 'should_brake': True, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0006.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.002,-0.0) \\n             Your current speed is -0.0 m/s \\n             Your current angle is 6.283  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-175.248,-3.457) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.007, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0009.png', 'x': -0.001610136389302852, 'y': -3.104682434048109e-05, 'theta': 6.282840251922607, 'speed': -1.4144580943306382e-08, 'x_command_near': -175.24763159398083, 'y_command_near': -3.4570198106200576, 'command_near': 4, 'should_brake': True, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0009.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.002,-0.0) \\n             Your current speed is -0.0 m/s \\n             Your current angle is 6.283  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-175.248,-3.457) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.014, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0012.png', 'x': -0.001610136389302852, 'y': -3.104682434048109e-05, 'theta': 6.282840251922607, 'speed': -1.0749678139539583e-08, 'x_command_near': -175.24763159398083, 'y_command_near': -3.4570198106200576, 'command_near': 4, 'should_brake': True, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0012.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.002,-0.0) \\n             Your current speed is -0.0 m/s \\n             Your current angle is 6.283  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-175.248,-3.457) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.007, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0015.png', 'x': -0.001610136389302852, 'y': -3.104682434048109e-05, 'theta': 6.282840251922607, 'speed': -4.508208445426863e-09, 'x_command_near': -175.24763159398083, 'y_command_near': -3.4570198106200576, 'command_near': 4, 'should_brake': True, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0015.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.002,-0.0) \\n             Your current speed is -0.0 m/s \\n             Your current angle is 6.283  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-175.248,-3.457) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.003, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0018.png', 'x': -0.001610136389302852, 'y': -3.104682434048109e-05, 'theta': 6.282840251922607, 'speed': 1.9724780467890113e-08, 'x_command_near': -175.24763159398083, 'y_command_near': -3.4570198106200576, 'command_near': 4, 'should_brake': True, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0018.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.002,-0.0) \\n             Your current speed is 0.0 m/s \\n             Your current angle is 6.283  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-175.248,-3.457) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.009, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0021.png', 'x': -0.001610136389302852, 'y': -3.104682434048109e-05, 'theta': 6.282840251922607, 'speed': -5.487694042322396e-10, 'x_command_near': -175.24763159398083, 'y_command_near': -3.4570198106200576, 'command_near': 4, 'should_brake': True, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0021.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.002,-0.0) \\n             Your current speed is -0.0 m/s \\n             Your current angle is 6.283  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-175.248,-3.457) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.006, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0024.png', 'x': -0.001610136389302852, 'y': -3.104682434048109e-05, 'theta': 6.282840251922607, 'speed': 4.529610073041418e-08, 'x_command_near': -175.24763159398083, 'y_command_near': -3.4570198106200576, 'command_near': 4, 'should_brake': True, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0024.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.002,-0.0) \\n             Your current speed is 0.0 m/s \\n             Your current angle is 6.283  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-175.248,-3.457) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.013, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0027.png', 'x': -0.001610136389302852, 'y': -3.104682434048109e-05, 'theta': 6.282840251922607, 'speed': -5.2157524472011e-09, 'x_command_near': -175.24763159398083, 'y_command_near': -3.4570198106200576, 'command_near': 4, 'should_brake': True, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0027.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.002,-0.0) \\n             Your current speed is -0.0 m/s \\n             Your current angle is 6.283  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-175.248,-3.457) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.009, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0030.png', 'x': -0.001610136389302852, 'y': -3.104682434048109e-05, 'theta': 6.282840251922607, 'speed': -2.3022338968409578e-09, 'x_command_near': -175.24763159398083, 'y_command_near': -3.4570198106200576, 'command_near': 4, 'should_brake': True, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0030.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.002,-0.0) \\n             Your current speed is -0.0 m/s \\n             Your current angle is 6.283  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-175.248,-3.457) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.007, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0033.png', 'x': -0.001610136389302852, 'y': -3.104682434048109e-05, 'theta': 6.282840251922607, 'speed': 1.531850542558787e-08, 'x_command_near': -175.24763159398083, 'y_command_near': -3.4570198106200576, 'command_near': 4, 'should_brake': True, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0033.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.002,-0.0) \\n             Your current speed is 0.0 m/s \\n             Your current angle is 6.283  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-175.248,-3.457) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.012, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0036.png', 'x': -0.001610136389302852, 'y': -3.104682434048109e-05, 'theta': 6.282840251922607, 'speed': 1.1688277024494127e-08, 'x_command_near': -175.24763159398083, 'y_command_near': -3.4570198106200576, 'command_near': 4, 'should_brake': True, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0036.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.002,-0.0) \\n             Your current speed is 0.0 m/s \\n             Your current angle is 6.283  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-175.248,-3.457) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.012, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0039.png', 'x': -0.001610136389302852, 'y': -3.104682434048109e-05, 'theta': 6.282840251922607, 'speed': 6.138069672244852e-09, 'x_command_near': -175.24763159398083, 'y_command_near': -3.4570198106200576, 'command_near': 4, 'should_brake': True, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0039.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.002,-0.0) \\n             Your current speed is 0.0 m/s \\n             Your current angle is 6.283  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-175.248,-3.457) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.012, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0042.png', 'x': -0.001610136389302852, 'y': -3.104682434048109e-05, 'theta': 6.282840251922607, 'speed': -3.0461459774107475e-08, 'x_command_near': -175.24763159398083, 'y_command_near': -3.4570198106200576, 'command_near': 4, 'should_brake': True, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0042.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.002,-0.0) \\n             Your current speed is -0.0 m/s \\n             Your current angle is 6.283  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-175.248,-3.457) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.009, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0045.png', 'x': -0.001610136389302852, 'y': -3.104682434048109e-05, 'theta': 6.282840251922607, 'speed': -2.2171840038129748e-08, 'x_command_near': -175.24763159398083, 'y_command_near': -3.4570198106200576, 'command_near': 4, 'should_brake': True, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0045.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.002,-0.0) \\n             Your current speed is -0.0 m/s \\n             Your current angle is 6.283  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-175.248,-3.457) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.013, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0048.png', 'x': -0.001610136389302852, 'y': -3.104682434048109e-05, 'theta': 6.282840251922607, 'speed': 6.098642283692591e-09, 'x_command_near': -175.24763159398083, 'y_command_near': -3.4570198106200576, 'command_near': 4, 'should_brake': True, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0048.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.002,-0.0) \\n             Your current speed is 0.0 m/s \\n             Your current angle is 6.283  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-175.248,-3.457) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.012, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0051.png', 'x': -0.001610136389302852, 'y': -3.104682434048109e-05, 'theta': 6.282840251922607, 'speed': 6.866002890973261e-09, 'x_command_near': -175.24763159398083, 'y_command_near': -3.4570198106200576, 'command_near': 4, 'should_brake': True, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0051.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.002,-0.0) \\n             Your current speed is 0.0 m/s \\n             Your current angle is 6.283  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-175.248,-3.457) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.013, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0054.png', 'x': -0.001610136389302852, 'y': -3.104682434048109e-05, 'theta': 6.282840251922607, 'speed': 0.0, 'x_command_near': -175.24763159398083, 'y_command_near': -3.4570198106200576, 'command_near': 4, 'should_brake': True, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0054.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.002,-0.0) \\n             Your current speed is 0.0 m/s \\n             Your current angle is 6.283  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-175.248,-3.457) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.009, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0057.png', 'x': -0.001610136389302852, 'y': -3.104682434048109e-05, 'theta': 6.282840251922607, 'speed': -2.676275040289807e-09, 'x_command_near': -175.24763159398083, 'y_command_near': -3.4570198106200576, 'command_near': 4, 'should_brake': True, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0057.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.002,-0.0) \\n             Your current speed is -0.0 m/s \\n             Your current angle is 6.283  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-175.248,-3.457) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.009, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0060.png', 'x': -0.001610136389302852, 'y': -3.104682434048109e-05, 'theta': 6.282840251922607, 'speed': -2.4203795502560067e-09, 'x_command_near': -175.24763159398083, 'y_command_near': -3.4570198106200576, 'command_near': 4, 'should_brake': True, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0060.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.002,-0.0) \\n             Your current speed is -0.0 m/s \\n             Your current angle is 6.283  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-175.248,-3.457) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.009, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0063.png', 'x': -0.001610136389302852, 'y': -3.104682434048109e-05, 'theta': 6.282840251922607, 'speed': 5.380054123967203e-10, 'x_command_near': -175.24763159398083, 'y_command_near': -3.4570198106200576, 'command_near': 4, 'should_brake': True, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0063.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.002,-0.0) \\n             Your current speed is 0.0 m/s \\n             Your current angle is 6.283  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-175.248,-3.457) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.006, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0066.png', 'x': -0.001610136389302852, 'y': -3.104682434048109e-05, 'theta': 6.282840251922607, 'speed': 1.2397919923144601e-09, 'x_command_near': -175.24763159398083, 'y_command_near': -3.4570198106200576, 'command_near': 4, 'should_brake': True, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0066.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.002,-0.0) \\n             Your current speed is 0.0 m/s \\n             Your current angle is 6.283  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-175.248,-3.457) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.013, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0069.png', 'x': -0.001610136389302852, 'y': -3.104682434048109e-05, 'theta': 6.282840251922607, 'speed': 6.558177852238722e-09, 'x_command_near': -175.24763159398083, 'y_command_near': -3.4570198106200576, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0069.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.002,-0.0) \\n             Your current speed is 0.0 m/s \\n             Your current angle is 6.283  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-175.248,-3.457) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : 0.056, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0072.png', 'x': -0.001610136389302852, 'y': -3.104682434048109e-05, 'theta': 6.282840251922607, 'speed': 3.791999726823152e-09, 'x_command_near': -175.24763159398083, 'y_command_near': -3.4570198106200576, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0072.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.002,-0.0) \\n             Your current speed is 0.0 m/s \\n             Your current angle is 6.283  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-175.248,-3.457) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : 0.074, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0075.png', 'x': -0.001610136389302852, 'y': -3.104682434048109e-05, 'theta': 6.282840251922607, 'speed': 4.6018780296890115e-09, 'x_command_near': -175.24763159398083, 'y_command_near': -3.4570198106200576, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0075.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.002,-0.0) \\n             Your current speed is 0.0 m/s \\n             Your current angle is 6.283  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-175.248,-3.457) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : 0.061, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0078.png', 'x': -0.001610136389302852, 'y': -3.104682434048109e-05, 'theta': 6.282840251922607, 'speed': 4.845825403100475e-09, 'x_command_near': -175.24763159398083, 'y_command_near': -3.4570198106200576, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0078.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.002,-0.0) \\n             Your current speed is 0.0 m/s \\n             Your current angle is 6.283  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-175.248,-3.457) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : 0.074, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0081.png', 'x': -0.001610136389302852, 'y': -3.104682434048109e-05, 'theta': 6.282840251922607, 'speed': 1.421502254541193e-09, 'x_command_near': -175.24763159398083, 'y_command_near': -3.4570198106200576, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0081.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.002,-0.0) \\n             Your current speed is 0.0 m/s \\n             Your current angle is 6.283  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-175.248,-3.457) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : 0.05, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0084.png', 'x': -0.001610136389302852, 'y': -3.104682434048109e-05, 'theta': 6.282840251922607, 'speed': 7.199800816194624e-09, 'x_command_near': -175.24763159398083, 'y_command_near': -3.4570198106200576, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0084.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.002,-0.0) \\n             Your current speed is 0.0 m/s \\n             Your current angle is 6.283  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-175.248,-3.457) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.016, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0087.png', 'x': -0.0016101048627348291, 'y': -3.104684575798645e-05, 'theta': 6.2831854820251465, 'speed': 0.1383570951032926, 'x_command_near': -174.24758563795888, 'y_command_near': -3.457245831433354, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0087.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.002,-0.0) \\n             Your current speed is 0.138 m/s \\n             Your current angle is 6.283  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-174.248,-3.457) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : 0.009, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0090.png', 'x': -0.0016094445867480545, 'y': -3.1046717252954276e-05, 'theta': 6.282840251922607, 'speed': 0.7941601249808795, 'x_command_near': -174.24758563795888, 'y_command_near': -3.457245831433354, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0090.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.002,-0.0) \\n             Your current speed is 0.794 m/s \\n             Your current angle is 6.283  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-174.248,-3.457) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : 0.007, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0093.png', 'x': -0.0016078612676864168, 'y': -3.10475932289236e-05, 'theta': 6.282840251922607, 'speed': 1.4839287779344084, 'x_command_near': -174.24758563795888, 'y_command_near': -3.457245831433354, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0093.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.002,-0.0) \\n             Your current speed is 1.484 m/s \\n             Your current angle is 6.283  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-174.248,-3.457) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : 0.018, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0096.png', 'x': -0.001605291715321755, 'y': -3.104737262861837e-05, 'theta': 6.282840251922607, 'speed': 2.218121543061732, 'x_command_near': -174.24758563795888, 'y_command_near': -3.457245831433354, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0096.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.002,-0.0) \\n             Your current speed is 2.218 m/s \\n             Your current angle is 6.283  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-174.248,-3.457) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.012, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0099.png', 'x': -0.0016018199548426537, 'y': -3.1048921114256075e-05, 'theta': 6.2826972007751465, 'speed': 2.8562593924981536, 'x_command_near': -174.24758563795888, 'y_command_near': -3.457245831433354, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0099.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.002,-0.0) \\n             Your current speed is 2.856 m/s \\n             Your current angle is 6.283  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-174.248,-3.457) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.009, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0102.png', 'x': -0.0015975110954542515, 'y': -3.104962146668143e-05, 'theta': 6.282840251922607, 'speed': 3.4695200603825107, 'x_command_near': -173.2475396803549, 'y_command_near': -3.4574713754094915, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0102.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.002,-0.0) \\n             Your current speed is 3.47 m/s \\n             Your current angle is 6.283  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-173.248,-3.457) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.016, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0105.png', 'x': -0.0015923844642884433, 'y': -3.1050891524749426e-05, 'theta': 6.2826972007751465, 'speed': 4.070836628791881, 'x_command_near': -173.2475396803549, 'y_command_near': -3.4574713754094915, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0105.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.002,-0.0) \\n             Your current speed is 4.071 m/s \\n             Your current angle is 6.283  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-173.248,-3.457) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.017, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0108.png', 'x': -0.00158645719538697, 'y': -3.1052566373668776e-05, 'theta': 6.2831854820251465, 'speed': 4.659911312654699, 'x_command_near': -172.24749372433294, 'y_command_near': -3.457697396222788, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0108.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.002,-0.0) \\n             Your current speed is 4.66 m/s \\n             Your current angle is 6.283  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-172.247,-3.458) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : 0.022, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0111.png', 'x': -0.0015797446407930238, 'y': -3.104646880989207e-05, 'theta': 0.00048828125, 'speed': 5.237467191278116, 'x_command_near': -171.247447768311, 'y_command_near': -3.4579229401989253, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0111.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.002,-0.0) \\n             Your current speed is 5.237 m/s \\n             Your current angle is 0.0  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-171.247,-3.458) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.957, 'steer' : 0.01, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0114.png', 'x': -0.001572264756944719, 'y': -3.104386872474106e-05, 'theta': 0.0, 'speed': 5.7980468357039605, 'x_command_near': -170.24740181387105, 'y_command_near': -3.4581489610122214, 'command_near': 4, 'should_brake': True, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0114.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.002,-0.0) \\n             Your current speed is 5.798 m/s \\n             Your current angle is 0.0  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-170.247,-3.458) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.013, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0117.png', 'x': -0.0015640838867341245, 'y': -3.1050951493764444e-05, 'theta': 6.28258752822876, 'speed': 6.230660242519003, 'x_command_near': -169.24735585626706, 'y_command_near': -3.458374504988359, 'command_near': 4, 'should_brake': True, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0117.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.002,-0.0) \\n             Your current speed is 6.231 m/s \\n             Your current angle is 6.283  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-169.247,-3.458) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.009, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0120.png', 'x': -0.0015567975486163732, 'y': -3.105553269816148e-05, 'theta': 6.2826972007751465, 'speed': 4.594765772478666, 'x_command_near': -169.24735585626706, 'y_command_near': -3.458374504988359, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0120.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.002,-0.0) \\n             Your current speed is 4.595 m/s \\n             Your current angle is 6.283  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-169.247,-3.458) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.006, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0123.png', 'x': -0.0015520096224435065, 'y': -3.105317463082107e-05, 'theta': 6.282840251922607, 'speed': 2.86332619804788, 'x_command_near': -168.2473099002451, 'y_command_near': -3.4586005258016552, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0123.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.002,-0.0) \\n             Your current speed is 2.863 m/s \\n             Your current angle is 6.283  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-168.247,-3.459) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.014, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0126.png', 'x': -0.0015481485773847226, 'y': -3.1055950339516046e-05, 'theta': 6.28258752822876, 'speed': 2.8585905603288437, 'x_command_near': -168.2473099002451, 'y_command_near': -3.4586005258016552, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0126.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.002,-0.0) \\n             Your current speed is 2.859 m/s \\n             Your current angle is 6.283  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-168.247,-3.459) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.022, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0129.png', 'x': -0.0015438412257822165, 'y': -3.10580449715405e-05, 'theta': 6.282840251922607, 'speed': 3.608792149807854, 'x_command_near': -167.24726394422314, 'y_command_near': -3.4588260697777926, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0129.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.002,-0.0) \\n             Your current speed is 3.609 m/s \\n             Your current angle is 6.283  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-167.247,-3.459) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.009, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0132.png', 'x': -0.0015383625936351564, 'y': -3.1054502516153544e-05, 'theta': 0.0, 'speed': 4.39419988927211, 'x_command_near': -167.24726394422314, 'y_command_near': -3.4588260697777926, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0132.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.002,-0.0) \\n             Your current speed is 4.394 m/s \\n             Your current angle is 0.0  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-167.247,-3.459) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.022, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0135.png', 'x': -0.0015319485819418333, 'y': -3.1055072221796184e-05, 'theta': 6.282840251922607, 'speed': 5.036770537915844, 'x_command_near': -166.2472179882012, 'y_command_near': -3.459052090591089, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0135.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.002,-0.0) \\n             Your current speed is 5.037 m/s \\n             Your current angle is 6.283  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-166.247,-3.459) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.892, 'steer' : 0.017, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0138.png', 'x': -0.001524729409126735, 'y': -3.1063324386612356e-05, 'theta': 6.282413482666016, 'speed': 5.600397677642639, 'x_command_near': -165.24717203217924, 'y_command_near': -3.4592776345672265, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0138.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.002,-0.0) \\n             Your current speed is 5.6 m/s \\n             Your current angle is 6.282  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-165.247,-3.459) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.862, 'steer' : 0.017, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0141.png', 'x': -0.001516815966425611, 'y': -3.1068779425228184e-05, 'theta': 6.2826972007751465, 'speed': 6.08161509835552, 'x_command_near': -164.2471260761573, 'y_command_near': -3.4595036553805234, 'command_near': 4, 'should_brake': True, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0141.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.002,-0.0) \\n             Your current speed is 6.082 m/s \\n             Your current angle is 6.283  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-164.247,-3.46) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.017, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0144.png', 'x': -0.0015083482044531138, 'y': -3.1064508774658904e-05, 'theta': 0.0, 'speed': 6.35525643735936, 'x_command_near': -163.2470801185533, 'y_command_near': -3.4597291993566603, 'command_near': 4, 'should_brake': True, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0144.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.002,-0.0) \\n             Your current speed is 6.355 m/s \\n             Your current angle is 0.0  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-163.247,-3.46) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.01, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0147.png', 'x': -0.0015001710351754127, 'y': -3.106245483589464e-05, 'theta': 6.2831854820251465, 'speed': 5.748945528518591, 'x_command_near': -162.2487127087411, 'y_command_near': -3.4597783135840228, 'command_near': 4, 'should_brake': True, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0147.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.002,-0.0) \\n             Your current speed is 5.749 m/s \\n             Your current angle is 6.283  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-162.249,-3.46) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.009, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0150.png', 'x': -0.00149314554512614, 'y': -3.106716668707439e-05, 'theta': 6.282840251922607, 'speed': 4.642803046236082, 'x_command_near': -161.25194754283766, 'y_command_near': -3.458206419889863, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0150.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 4.643 m/s \\n             Your current angle is 6.283  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-161.252,-3.458) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.013, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0153.png', 'x': -0.001488292783889733, 'y': -3.106822042833822e-05, 'theta': 6.2831854820251465, 'speed': 2.9143218744552493, 'x_command_near': -161.25194754283766, 'y_command_near': -3.458206419889863, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0153.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 2.914 m/s \\n             Your current angle is 6.283  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-161.252,-3.458) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.017, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0156.png', 'x': -0.0014841717131872656, 'y': -3.1062591907928956e-05, 'theta': 0.0003452669770922512, 'speed': 3.196237231771066, 'x_command_near': -160.2519015868157, 'y_command_near': -3.4559586095230297, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0156.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 3.196 m/s \\n             Your current angle is 0.0  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-160.252,-3.456) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.004, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0159.png', 'x': -0.0014795464915380308, 'y': -3.105420909633008e-05, 'theta': 0.000845727976411581, 'speed': 3.6248726941926064, 'x_command_near': -160.2519015868157, 'y_command_near': -3.4559586095230297, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0159.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 3.625 m/s \\n             Your current angle is 0.001  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-160.252,-3.456) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.017, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0162.png', 'x': -0.0014743022525749439, 'y': -3.104540007637448e-05, 'theta': 0.0011451210593804717, 'speed': 4.1055108333581005, 'x_command_near': -159.25185563079373, 'y_command_near': -3.4537103223190377, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0162.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 4.106 m/s \\n             Your current angle is 0.001  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-159.252,-3.454) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.012, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0165.png', 'x': -0.0014683945849753854, 'y': -3.10345242671514e-05, 'theta': 0.0015049839857965708, 'speed': 4.607277586035372, 'x_command_near': -159.25185563079373, 'y_command_near': -3.4537103223190377, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0165.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 4.607 m/s \\n             Your current angle is 0.002  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-159.252,-3.454) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.016, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0168.png', 'x': -0.0014618026537505102, 'y': -3.101960054941481e-05, 'theta': 0.0019223655108362436, 'speed': 5.120482774146241, 'x_command_near': -158.25182493333355, 'y_command_near': -3.451462035115046, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0168.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 5.12 m/s \\n             Your current angle is 0.002  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-158.252,-3.451) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.958, 'steer' : 0.006, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0171.png', 'x': -0.0014545163156611807, 'y': -3.100655514689852e-05, 'theta': 0.0015440810238942504, 'speed': 5.634758549893866, 'x_command_near': -157.2517789788936, 'y_command_near': -3.449213747911054, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0171.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 5.635 m/s \\n             Your current angle is 0.002  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-157.252,-3.449) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.519, 'steer' : -0.017, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0174.png', 'x': -0.0014470024379278357, 'y': -3.099380744770676e-05, 'theta': 0.0019531252328306437, 'speed': 5.525390495843223, 'x_command_near': -156.25173302287166, 'y_command_near': -3.446965460707062, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0174.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 5.525 m/s \\n             Your current angle is 0.002  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-156.252,-3.447) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.779, 'steer' : -0.014, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0177.png', 'x': -0.0014394896568035165, 'y': -3.0973788505444426e-05, 'theta': 0.0023670317605137825, 'speed': 5.617771717251292, 'x_command_near': -155.2516870668497, 'y_command_near': -3.4447176503402286, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0177.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 5.618 m/s \\n             Your current angle is 0.002  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-155.252,-3.445) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.877, 'steer' : -0.024, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0180.png', 'x': -0.0014320085393251247, 'y': -3.095144362209973e-05, 'theta': 0.0027404725551605225, 'speed': 5.500323934144895, 'x_command_near': -155.2516870668497, 'y_command_near': -3.4447176503402286, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0180.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 5.5 m/s \\n             Your current angle is 0.003  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-155.252,-3.445) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.844, 'steer' : -0.008, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0183.png', 'x': -0.0014243127670283684, 'y': -3.093097062872372e-05, 'theta': 0.00248975632712245, 'speed': 5.902357391511645, 'x_command_near': -154.25164110924572, 'y_command_near': -3.442469363136236, 'command_near': 4, 'should_brake': True, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0183.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 5.902 m/s \\n             Your current angle is 0.002  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-154.252,-3.442) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.022, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0186.png', 'x': -0.001416151498105478, 'y': -3.091552004035521e-05, 'theta': 0.0020716022700071335, 'speed': 6.060850947606548, 'x_command_near': -153.25159515322378, 'y_command_near': -3.4402210759322442, 'command_near': 4, 'should_brake': True, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0186.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 6.061 m/s \\n             Your current angle is 0.002  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-153.252,-3.44) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.013, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0189.png', 'x': -0.0014085252213220656, 'y': -3.08995018880946e-05, 'theta': 0.002316121943295002, 'speed': 5.119746860411105, 'x_command_near': -152.2515644573456, 'y_command_near': -3.4379727887282523, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0189.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 5.12 m/s \\n             Your current angle is 0.002  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-152.252,-3.438) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.967, 'steer' : 0.017, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0192.png', 'x': -0.0014030319225071253, 'y': -3.0883468743580235e-05, 'theta': 0.002583742141723633, 'speed': 3.3868459639432302, 'x_command_near': -151.25151850290567, 'y_command_near': -3.4357245015242603, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0192.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 3.387 m/s \\n             Your current angle is 0.003  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-151.252,-3.436) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.017, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0195.png', 'x': -0.0013983060899676047, 'y': -3.087415641224862e-05, 'theta': 0.0020716022700071335, 'speed': 3.6170073072959616, 'x_command_near': -151.25151850290567, 'y_command_near': -3.4357245015242603, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0195.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 3.617 m/s \\n             Your current angle is 0.002  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-151.252,-3.436) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.022, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0198.png', 'x': -0.0013931869977881206, 'y': -3.0869845068419144e-05, 'theta': 0.0016558425268158317, 'speed': 3.9446341755692833, 'x_command_near': -150.2514725453017, 'y_command_near': -3.4334762143202684, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0198.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 3.945 m/s \\n             Your current angle is 0.002  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-150.251,-3.433) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.016, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0201.png', 'x': -0.0013876017235929794, 'y': -3.086110886798178e-05, 'theta': 0.0017940601101145148, 'speed': 4.303712707902597, 'x_command_near': -150.2514725453017, 'y_command_near': -3.4334762143202684, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0201.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 4.304 m/s \\n             Your current angle is 0.002  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-150.251,-3.433) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.016, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0204.png', 'x': -0.0013815254573898983, 'y': -3.0846116614228035e-05, 'theta': 0.002128368942067027, 'speed': 4.673291175943906, 'x_command_near': -149.25142658927973, 'y_command_near': -3.431228403953435, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0204.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 4.673 m/s \\n             Your current angle is 0.002  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-149.251,-3.431) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.021, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0207.png', 'x': -0.0013749481929039575, 'y': -3.0827309902769226e-05, 'theta': 0.002560569206252694, 'speed': 5.0475095248407, 'x_command_near': -148.2513806348398, 'y_command_near': -3.4289801167494423, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0207.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 5.048 m/s \\n             Your current angle is 0.003  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-148.251,-3.429) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.889, 'steer' : 0.017, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0210.png', 'x': -0.0013678703413404492, 'y': -3.081019089073298e-05, 'theta': 0.002210787730291486, 'speed': 5.4110045684028565, 'x_command_near': -148.2513806348398, 'y_command_near': -3.4289801167494423, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0210.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 5.411 m/s \\n             Your current angle is 0.002  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-148.251,-3.429) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.898, 'steer' : -0.007, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0213.png', 'x': -0.001360330282878408, 'y': -3.079882890413817e-05, 'theta': 0.0017940601101145148, 'speed': 5.741074458369247, 'x_command_near': -147.25133467881784, 'y_command_near': -3.4267318295454503, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0213.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 5.741 m/s \\n             Your current angle is 0.002  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-147.251,-3.427) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.667, 'steer' : -0.018, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0216.png', 'x': -0.001352363519160349, 'y': -3.078408295169609e-05, 'theta': 0.0020716022700071335, 'speed': 6.0369678353273795, 'x_command_near': -146.25130397977566, 'y_command_near': -3.4244835423414584, 'command_near': 4, 'should_brake': True, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0216.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 6.037 m/s \\n             Your current angle is 0.002  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-146.251,-3.424) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.014, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0219.png', 'x': -0.0013441459136345202, 'y': -3.07612004889668e-05, 'theta': 0.0025371843948960304, 'speed': 6.023909347045512, 'x_command_near': -145.25125802533572, 'y_command_near': -3.4222352551374664, 'command_near': 4, 'should_brake': True, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0219.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 6.024 m/s \\n             Your current angle is 0.003  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-145.251,-3.422) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.012, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0222.png', 'x': -0.00133697238580055, 'y': -3.074837568675573e-05, 'theta': 0.004011642653495073, 'speed': 4.560191906293862, 'x_command_near': -144.25121206931374, 'y_command_near': -3.4199869679334745, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0222.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 4.56 m/s \\n             Your current angle is 0.004  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-144.251,-3.42) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.024, 'brake': 0.038 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0225.png', 'x': -0.001332253269794137, 'y': -3.0735867863624034e-05, 'theta': 0.003686444601044059, 'speed': 2.6714395994414137, 'x_command_near': -144.25121206931374, 'y_command_near': -3.4199869679334745, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0225.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 2.671 m/s \\n             Your current angle is 0.004  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-144.251,-3.42) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.013, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0228.png', 'x': -0.001329427666888705, 'y': -3.073044280951571e-05, 'theta': 0.003257243661209941, 'speed': 2.0137574911854106, 'x_command_near': -143.25116611170978, 'y_command_near': -3.417739157566641, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0228.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 2.014 m/s \\n             Your current angle is 0.003  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-143.251,-3.418) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.018, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0231.png', 'x': -0.001326399882728424, 'y': -3.072634992424094e-05, 'theta': 0.0027836323715746403, 'speed': 2.438024630454153, 'x_command_near': -143.25116611170978, 'y_command_near': -3.417739157566641, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0231.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 2.438 m/s \\n             Your current angle is 0.003  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-143.251,-3.418) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.017, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0234.png', 'x': -0.0013227548631959962, 'y': -3.0720515795780196e-05, 'theta': 0.0025135823525488377, 'speed': 2.9155686278260533, 'x_command_near': -143.25116611170978, 'y_command_near': -3.417739157566641, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0234.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 2.916 m/s \\n             Your current angle is 0.003  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-143.251,-3.418) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.016, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0237.png', 'x': -0.0013184546393318897, 'y': -3.071284404535935e-05, 'theta': 0.0022902425844222307, 'speed': 3.4104017081831386, 'x_command_near': -142.25112015726984, 'y_command_near': -3.4154908703626483, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0237.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 3.41 m/s \\n             Your current angle is 0.002  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-142.251,-3.415) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.013, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0240.png', 'x': -0.0013134820771512068, 'y': -3.070522369695137e-05, 'theta': 0.0019531252328306437, 'speed': 3.9138814867046836, 'x_command_near': -141.25107420124786, 'y_command_near': -3.4132425831586564, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0240.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 3.914 m/s \\n             Your current angle is 0.002  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-141.251,-3.413) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.024, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0243.png', 'x': -0.0013078268962090078, 'y': -3.069332198922143e-05, 'theta': 0.002264067530632019, 'speed': 4.423508696777521, 'x_command_near': -141.25107420124786, 'y_command_near': -3.4132425831586564, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0243.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 4.424 m/s \\n             Your current angle is 0.002  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-141.251,-3.413) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.026, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0246.png', 'x': -0.0013014812834626355, 'y': -3.067366286104919e-05, 'theta': 0.0026966221630573273, 'speed': 4.938574518620754, 'x_command_near': -140.25104350220568, 'y_command_near': -3.4109942959546644, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0246.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 4.939 m/s \\n             Your current angle is 0.003  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-140.251,-3.411) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.962, 'steer' : -0.017, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0249.png', 'x': -0.0012944390706337572, 'y': -3.065561432928021e-05, 'theta': 0.0023417151533067226, 'speed': 5.454044244356834, 'x_command_near': -139.25099754776573, 'y_command_near': -3.4087460087506725, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0249.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 5.454 m/s \\n             Your current angle is 0.002  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-139.251,-3.409) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.888, 'steer' : -0.022, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0252.png', 'x': -0.0012867182141462763, 'y': -3.0644237350431654e-05, 'theta': 0.00172633514739573, 'speed': 5.944102836397264, 'x_command_near': -138.25095159174379, 'y_command_near': -3.4064977215466805, 'command_near': 4, 'should_brake': True, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0252.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 5.944 m/s \\n             Your current angle is 0.002  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-138.251,-3.406) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.024, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0255.png', 'x': -0.0012784215180658975, 'y': -3.06288788573362e-05, 'theta': 0.002156191971153021, 'speed': 6.237519091917654, 'x_command_near': -138.25095159174379, 'y_command_near': -3.4064977215466805, 'command_near': 4, 'should_brake': True, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0255.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 6.238 m/s \\n             Your current angle is 0.002  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-138.251,-3.406) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.015, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0258.png', 'x': -0.001270394305578293, 'y': -3.060554448524376e-05, 'theta': 0.0026294756680727005, 'speed': 5.641953930014974, 'x_command_near': -137.25090563572184, 'y_command_near': -3.404249911179847, 'command_near': 4, 'should_brake': True, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0258.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 5.642 m/s \\n             Your current angle is 0.003  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-137.251,-3.404) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.015, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0261.png', 'x': -0.00126363514651473, 'y': -3.058163184050653e-05, 'theta': 0.001035801018588245, 'speed': 4.314541382698076, 'x_command_near': -136.2508596812819, 'y_command_near': -3.402001623975855, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0261.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 4.315 m/s \\n             Your current angle is 0.001  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-136.251,-3.402) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.022, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0264.png', 'x': -0.0012592190967808392, 'y': -3.0572559385234976e-05, 'theta': 0.0012918708380311728, 'speed': 2.5922656994422963, 'x_command_near': -135.2508137236779, 'y_command_near': -3.399753336771863, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0264.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 2.592 m/s \\n             Your current angle is 0.001  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-135.251,-3.4) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.012, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0267.png', 'x': -0.0012557223892031288, 'y': -3.056228754966312e-05, 'theta': 0.0017605233006179333, 'speed': 2.58810398713504, 'x_command_near': -135.2508137236779, 'y_command_near': -3.399753336771863, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0267.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 2.588 m/s \\n             Your current angle is 0.002  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-135.251,-3.4) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.027, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0270.png', 'x': -0.001252269681728535, 'y': -3.055073280552005e-05, 'theta': 0.002156191971153021, 'speed': 2.4990567259122853, 'x_command_near': -135.2508137236779, 'y_command_near': -3.399753336771863, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0270.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 2.499 m/s \\n             Your current angle is 0.002  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-135.251,-3.4) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.012, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0273.png', 'x': -0.0012487735224198104, 'y': -3.053977989327767e-05, 'theta': 0.0023920803796499968, 'speed': 2.728157094527437, 'x_command_near': -134.25078302621773, 'y_command_near': -3.397505049567871, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0273.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 2.728 m/s \\n             Your current angle is 0.002  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-134.251,-3.398) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.024, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0276.png', 'x': -0.001244756489370502, 'y': -3.0530473987197664e-05, 'theta': 0.002210787730291486, 'speed': 3.197104111388185, 'x_command_near': -134.25078302621773, 'y_command_near': -3.397505049567871, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0276.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 3.197 m/s \\n             Your current angle is 0.002  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-134.251,-3.398) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.023, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0279.png', 'x': -0.0012400418967501992, 'y': -3.052314277511208e-05, 'theta': 0.0019223655108362436, 'speed': 3.743497601104347, 'x_command_near': -133.25073706861374, 'y_command_near': -3.3952567623638785, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0279.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 3.743 m/s \\n             Your current angle is 0.002  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-133.251,-3.395) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.022, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0282.png', 'x': -0.0012345712147947552, 'y': -3.0511513069700244e-05, 'theta': 0.0022375863045454025, 'speed': 4.313091088475778, 'x_command_near': -133.25073706861374, 'y_command_near': -3.3952567623638785, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0282.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 4.313 m/s \\n             Your current angle is 0.002  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-133.251,-3.395) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.017, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0285.png', 'x': -0.001228330736282146, 'y': -3.049230370914062e-05, 'theta': 0.0026744273491203785, 'speed': 4.8846279260418335, 'x_command_near': -132.2506911141738, 'y_command_near': -3.3930084751598866, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0285.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 4.885 m/s \\n             Your current angle is 0.003  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-132.251,-3.393) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.978, 'steer' : -0.016, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0288.png', 'x': -0.00122132457339319, 'y': -3.047461070796066e-05, 'theta': 0.002264067530632019, 'speed': 5.448728577637656, 'x_command_near': -131.25064515815185, 'y_command_near': -3.3907606647930533, 'command_near': 4, 'should_brake': True, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0288.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 5.449 m/s \\n             Your current angle is 0.002  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-131.251,-3.391) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.009, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0291.png', 'x': -0.0012136041281536336, 'y': -3.04638719707719e-05, 'theta': 0.001691456069238484, 'speed': 5.895379687680478, 'x_command_near': -130.25059920371191, 'y_command_near': -3.3885123775890613, 'command_near': 4, 'should_brake': True, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0291.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 5.895 m/s \\n             Your current angle is 0.002  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-130.251,-3.389) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.013, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0294.png', 'x': -0.001205837900826623, 'y': -3.045020974410116e-05, 'theta': 0.002042627427726984, 'speed': 5.55179829505065, 'x_command_near': -129.25055324768996, 'y_command_near': -3.3862640903850694, 'command_near': 4, 'should_brake': True, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0294.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 5.552 m/s \\n             Your current angle is 0.002  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-129.251,-3.386) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.015, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0297.png', 'x': -0.0011989290590861401, 'y': -3.043123811785106e-05, 'theta': 0.0023920803796499968, 'speed': 4.757886030455981, 'x_command_near': -129.25055324768996, 'y_command_near': -3.3862640903850694, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0297.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 4.758 m/s \\n             Your current angle is 0.002  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-129.251,-3.386) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.022, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0300.png', 'x': -0.0011931677844074784, 'y': -3.041796997327899e-05, 'theta': 0.002156191971153021, 'speed': 3.971055652917555, 'x_command_near': -128.25050729008598, 'y_command_near': -3.384015803181077, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0300.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 3.971 m/s \\n             Your current angle is 0.002  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-128.251,-3.384) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.017, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0303.png', 'x': -0.001187951782270602, 'y': -3.041137552337788e-05, 'theta': 0.0017605233006179333, 'speed': 3.882274708167132, 'x_command_near': -127.25046896413592, 'y_command_near': -3.381767515977085, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0303.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 3.882 m/s \\n             Your current angle is 0.002  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-127.25,-3.382) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.017, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0306.png', 'x': -0.0011825277048131966, 'y': -3.0401748354717434e-05, 'theta': 0.0020132355857640505, 'speed': 4.090866513109999, 'x_command_near': -127.25046896413592, 'y_command_near': -3.381767515977085, 'command_near': 4, 'should_brake': False, 'x_target': -127.25046896413592, 'y_target': -3.381767515977085, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0306.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 4.091 m/s \\n             Your current angle is 0.002  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-127.25,-3.382) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-127.25,-3.382) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.012, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0309.png', 'x': -0.0011770211099815242, 'y': -3.0386854621488354e-05, 'theta': 0.0022902425844222307, 'speed': 4.082391736653208, 'x_command_near': -126.25042300811397, 'y_command_near': -3.3795197056102517, 'command_near': 4, 'should_brake': False, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0309.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 4.082 m/s \\n             Your current angle is 0.002  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-126.25,-3.38) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.024, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0312.png', 'x': -0.0011715265774938644, 'y': -3.0370849319730958e-05, 'theta': 0.002583742141723633, 'speed': 4.073911350825873, 'x_command_near': -126.25042300811397, 'y_command_near': -3.3795197056102517, 'command_near': 4, 'should_brake': False, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0312.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 4.074 m/s \\n             Your current angle is 0.003  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-126.25,-3.38) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.022, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0315.png', 'x': -0.001166043421960694, 'y': -3.0357214935817192e-05, 'theta': 0.0023670317605137825, 'speed': 4.065437866249587, 'x_command_near': -125.25037705367403, 'y_command_near': -3.3772714184062593, 'command_near': 4, 'should_brake': False, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0315.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 4.065 m/s \\n             Your current angle is 0.002  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-125.25,-3.377) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.017, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0318.png', 'x': -0.0011601685145876672, 'y': -3.0346810311712088e-05, 'theta': 0.0020716022700071335, 'speed': 4.602235465432455, 'x_command_near': -124.25033109607004, 'y_command_near': -3.3750231312022674, 'command_near': 4, 'should_brake': True, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0318.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 4.602 m/s \\n             Your current angle is 0.002  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-124.25,-3.375) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.014, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0321.png', 'x': -0.0011536238732361426, 'y': -3.0332642631914795e-05, 'theta': 0.0022902425844222307, 'speed': 4.963586152791009, 'x_command_near': -124.25033109607004, 'y_command_near': -3.3750231312022674, 'command_near': 4, 'should_brake': True, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0321.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 4.964 m/s \\n             Your current angle is 0.002  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-124.25,-3.375) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.013, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0324.png', 'x': -0.0011472939552419348, 'y': -3.0313696706671128e-05, 'theta': 0.002652046736329794, 'speed': 4.387766902590662, 'x_command_near': -123.25029277012, 'y_command_near': -3.3727748439982754, 'command_near': 4, 'should_brake': True, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0324.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 4.388 m/s \\n             Your current angle is 0.003  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-123.25,-3.373) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.023, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0327.png', 'x': -0.0011420617786086495, 'y': -3.0299539735626516e-05, 'theta': 0.0025371843948960304, 'speed': 3.4640937037683375, 'x_command_near': -122.25024681409803, 'y_command_near': -3.370526556794283, 'command_near': 4, 'should_brake': False, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0327.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 3.464 m/s \\n             Your current angle is 0.003  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-122.25,-3.371) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.024, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0330.png', 'x': -0.0011380335056685453, 'y': -3.0292634731897648e-05, 'theta': 0.002210787730291486, 'speed': 2.7575198976530357, 'x_command_near': -122.25024681409803, 'y_command_near': -3.370526556794283, 'command_near': 4, 'should_brake': False, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0330.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 2.758 m/s \\n             Your current angle is 0.002  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-122.25,-3.371) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.022, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0333.png', 'x': -0.001134267862724414, 'y': -3.028830411231335e-05, 'theta': 0.0018911054357886314, 'speed': 2.8815390519916404, 'x_command_near': -122.25024681409803, 'y_command_near': -3.370526556794283, 'command_near': 4, 'should_brake': False, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0333.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 2.882 m/s \\n             Your current angle is 0.002  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-122.25,-3.371) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.016, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0336.png', 'x': -0.0011301075208791644, 'y': -3.0280450313096922e-05, 'theta': 0.0021001773420721292, 'speed': 3.2659023847779727, 'x_command_near': -121.25020848656595, 'y_command_near': -3.368278269590291, 'command_near': 4, 'should_brake': False, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0336.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 3.266 m/s \\n             Your current angle is 0.002  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-121.25,-3.368) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.026, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0339.png', 'x': -0.0011253611960881926, 'y': -3.0267327807561318e-05, 'theta': 0.0023417151533067226, 'speed': 3.7305273696940806, 'x_command_near': -121.25020848656595, 'y_command_near': -3.368278269590291, 'command_near': 4, 'should_brake': False, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0339.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 3.731 m/s \\n             Your current angle is 0.002  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-121.25,-3.368) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.017, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0342.png', 'x': -0.00111996542399595, 'y': -3.0255490352347466e-05, 'theta': 0.0020716022700071335, 'speed': 4.224435567469334, 'x_command_near': -120.250162530544, 'y_command_near': -3.366030459223458, 'command_near': 4, 'should_brake': True, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0342.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 4.224 m/s \\n             Your current angle is 0.002  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-120.25,-3.366) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.006, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0345.png', 'x': -0.0011139583791646146, 'y': -3.0242029450227135e-05, 'theta': 0.0023670317605137825, 'speed': 4.54902900496574, 'x_command_near': -119.25011657452204, 'y_command_near': -3.3637821720194654, 'command_near': 4, 'should_brake': True, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0345.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 4.549 m/s \\n             Your current angle is 0.002  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-119.25,-3.364) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.014, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0348.png', 'x': -0.0011083674850311809, 'y': -3.0224368575305215e-05, 'theta': 0.0027404725551605225, 'speed': 3.7031914419834866, 'x_command_near': -119.25011657452204, 'y_command_near': -3.3637821720194654, 'command_near': 4, 'should_brake': False, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0348.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 3.703 m/s \\n             Your current angle is 0.003  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-119.25,-3.364) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.027, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0351.png', 'x': -0.0011040214791222525, 'y': -3.0212469009325813e-05, 'theta': 0.002583742141723633, 'speed': 3.0321288972663005, 'x_command_near': -117.56989409822748, 'y_command_near': -3.3600046680470315, 'command_near': 4, 'should_brake': False, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0351.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 3.032 m/s \\n             Your current angle is 0.003  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-117.57,-3.36) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.017, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0354.png', 'x': -0.001099937486387148, 'y': -3.0205360539296e-05, 'theta': 0.0022375863045454025, 'speed': 3.026032033663211, 'x_command_near': -117.56989409822748, 'y_command_near': -3.3600046680470315, 'command_near': 4, 'should_brake': False, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0354.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 3.026 m/s \\n             Your current angle is 0.002  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-117.57,-3.36) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.024, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0357.png', 'x': -0.0010958873504591793, 'y': -3.0199605655605095e-05, 'theta': 0.002042627427726984, 'speed': 2.96502189618326, 'x_command_near': -117.56989409822748, 'y_command_near': -3.3600046680470315, 'command_near': 4, 'should_brake': False, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0357.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 2.965 m/s \\n             Your current angle is 0.002  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-117.57,-3.36) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.027, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0360.png', 'x': -0.0010917492828070863, 'y': -3.019086945516773e-05, 'theta': 0.0021836606319993734, 'speed': 3.207800901013298, 'x_command_near': -117.56989409822748, 'y_command_near': -3.3600046680470315, 'command_near': 4, 'should_brake': False, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0360.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 3.208 m/s \\n             Your current angle is 0.002  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-117.57,-3.36) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.024, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0363.png', 'x': -0.0010870891763232748, 'y': -3.0177599168845126e-05, 'theta': 0.002416869392618537, 'speed': 3.673154771504841, 'x_command_near': -116.5698481406235, 'y_command_near': -3.3577563808430395, 'command_near': 4, 'should_brake': False, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0363.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 3.673 m/s \\n             Your current angle is 0.002  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-116.57,-3.358) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.017, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0366.png', 'x': -0.001081739391892711, 'y': -3.0165281961511153e-05, 'theta': 0.002210787730291486, 'speed': 4.211233047511806, 'x_command_near': -115.81461018763402, 'y_command_near': -3.3103897616683726, 'command_near': 4, 'should_brake': False, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0366.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 4.211 m/s \\n             Your current angle is 0.002  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-115.815,-3.31) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.022, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0369.png', 'x': -0.0010756468141011055, 'y': -3.013887417739919e-05, 'theta': 0.00402647303417325, 'speed': 4.769583360049144, 'x_command_near': -115.0717019040271, 'y_command_near': -3.1716594739274058, 'command_near': 4, 'should_brake': True, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0369.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 4.77 m/s \\n             Your current angle is 0.004  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-115.072,-3.172) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.015, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0372.png', 'x': -0.0010688339228011046, 'y': -3.0084163159950688e-05, 'theta': 0.0062530613504350185, 'speed': 5.226028477816556, 'x_command_near': -114.35117928951536, 'y_command_near': -2.943620685774796, 'command_near': 4, 'should_brake': True, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0372.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 5.226 m/s \\n             Your current angle is 0.006  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-114.351,-2.944) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.022, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0375.png', 'x': -0.0010619921778101116, 'y': -2.9967923933096795e-05, 'theta': 0.013986622914671898, 'speed': 4.7526243760474856, 'x_command_near': -113.66373925244905, 'y_command_near': -2.6296591794569797, 'command_near': 4, 'should_brake': True, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0375.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 4.753 m/s \\n             Your current angle is 0.014  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-113.664,-2.63) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.012, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0378.png', 'x': -0.0010570313352786798, 'y': -2.9750771846226417e-05, 'theta': 0.028059041127562523, 'speed': 2.849863267034085, 'x_command_near': -113.66373925244905, 'y_command_near': -2.6296591794569797, 'command_near': 4, 'should_brake': True, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0378.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 2.85 m/s \\n             Your current angle is 0.028  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-113.664,-2.63) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.012, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0381.png', 'x': -0.0010545874094560759, 'y': -2.9566116540242556e-05, 'theta': 0.03426496684551239, 'speed': 1.0108314682789863, 'x_command_near': -113.01956750059912, 'y_command_near': -2.23443413085311, 'command_near': 4, 'should_brake': True, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0381.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 1.011 m/s \\n             Your current angle is 0.034  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-113.02,-2.234) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.014, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0384.png', 'x': -0.001054242673291128, 'y': -2.953747062682021e-05, 'theta': 0.034421198070049286, 'speed': -0.0010255839505460868, 'x_command_near': -113.01956750059912, 'y_command_near': -2.23443413085311, 'command_near': 4, 'should_brake': True, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0384.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is -0.001 m/s \\n             Your current angle is 0.034  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-113.02,-2.234) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.013, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0387.png', 'x': -0.0010542360938217143, 'y': -2.9536755282141106e-05, 'theta': 0.03442639485001564, 'speed': -0.0009048828635542124, 'x_command_near': -113.01956750059912, 'y_command_near': -2.23443413085311, 'command_near': 4, 'should_brake': True, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0387.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is -0.001 m/s \\n             Your current angle is 0.034  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-113.02,-2.234) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.006, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0390.png', 'x': -0.0010542325299667255, 'y': -2.953620271050275e-05, 'theta': 0.034428127110004425, 'speed': -0.0004008839831067459, 'x_command_near': -113.01956750059912, 'y_command_near': -2.23443413085311, 'command_near': 4, 'should_brake': True, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0390.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is -0.0 m/s \\n             Your current angle is 0.034  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-113.02,-2.234) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.006, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0393.png', 'x': -0.001054231364847169, 'y': -2.9536029228709317e-05, 'theta': 0.034428127110004425, 'speed': -0.00016654713641763382, 'x_command_near': -113.01956750059912, 'y_command_near': -2.23443413085311, 'command_near': 4, 'should_brake': True, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0393.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is -0.0 m/s \\n             Your current angle is 0.034  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-113.02,-2.234) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.014, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0396.png', 'x': -0.0010542305424081633, 'y': -2.9535919999431967e-05, 'theta': 0.034428127110004425, 'speed': -8.669472887236315e-05, 'x_command_near': -113.01956750059912, 'y_command_near': -2.23443413085311, 'command_near': 4, 'should_brake': True, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0396.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is -0.0 m/s \\n             Your current angle is 0.034  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-113.02,-2.234) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.013, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0399.png', 'x': -0.001054230062649708, 'y': -2.9535881447922316e-05, 'theta': 0.034428127110004425, 'speed': -3.4286249359613304e-05, 'x_command_near': -113.01956750059912, 'y_command_near': -2.23443413085311, 'command_near': 4, 'should_brake': False, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0399.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is -0.0 m/s \\n             Your current angle is 0.034  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-113.02,-2.234) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.036, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0402.png', 'x': -0.0010542323243356577, 'y': -2.9536177009496317e-05, 'theta': 0.03463701903820038, 'speed': -0.015589382185198456, 'x_command_near': -113.01956750059912, 'y_command_near': -2.23443413085311, 'command_near': 4, 'should_brake': False, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0402.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is -0.016 m/s \\n             Your current angle is 0.035  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-113.02,-2.234) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.031, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0405.png', 'x': -0.001054177152838065, 'y': -2.9526973907442053e-05, 'theta': 0.03506297245621681, 'speed': 0.050351137461903624, 'x_command_near': -113.01956750059912, 'y_command_near': -2.23443413085311, 'command_near': 4, 'should_brake': False, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0405.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 0.05 m/s \\n             Your current angle is 0.035  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-113.02,-2.234) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.027, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0408.png', 'x': -0.0010538265225790155, 'y': -2.9473102456203756e-05, 'theta': 0.04080427438020706, 'speed': 0.49474228823458577, 'x_command_near': -113.01956750059912, 'y_command_near': -2.23443413085311, 'command_near': 4, 'should_brake': False, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0408.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 0.495 m/s \\n             Your current angle is 0.041  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-113.02,-2.234) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.04, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0411.png', 'x': -0.001052788270470728, 'y': -2.9317489287491448e-05, 'theta': 0.05407477915287018, 'speed': 1.1435483843508232, 'x_command_near': -113.01956750059912, 'y_command_near': -2.23443413085311, 'command_near': 4, 'should_brake': False, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0411.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 1.144 m/s \\n             Your current angle is 0.054  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-113.02,-2.234) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.037, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0414.png', 'x': -0.0010506317846790125, 'y': -2.8956056033996052e-05, 'theta': 0.07591023296117783, 'speed': 1.9475453110235514, 'x_command_near': -113.01956750059912, 'y_command_near': -2.23443413085311, 'command_near': 4, 'should_brake': False, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0414.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 1.948 m/s \\n             Your current angle is 0.076  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-113.02,-2.234) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.038, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0417.png', 'x': -0.0010475434832102337, 'y': -2.8391490592640574e-05, 'theta': 0.10327089577913284, 'speed': 2.6049667444328675, 'x_command_near': -112.42825462144042, 'y_command_near': -1.7638118291099756, 'command_near': 4, 'should_brake': False, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0417.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 2.605 m/s \\n             Your current angle is 0.103  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-112.428,-1.764) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.119, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0420.png', 'x': -0.0010436302822398602, 'y': -2.7607903316196564e-05, 'theta': 0.1335935741662979, 'speed': 3.2252126483423873, 'x_command_near': -111.89856719265069, 'y_command_near': -1.224778653860513, 'command_near': 4, 'should_brake': False, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0420.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 3.225 m/s \\n             Your current angle is 0.134  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-111.899,-1.225) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.131, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0423.png', 'x': -0.001039184282262795, 'y': -2.663298918084828e-05, 'theta': 0.16194914281368256, 'speed': 3.382694628017514, 'x_command_near': -111.89856719265069, 'y_command_near': -1.224778653860513, 'command_near': 4, 'should_brake': False, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0423.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 3.383 m/s \\n             Your current angle is 0.162  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-111.899,-1.225) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.126, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0426.png', 'x': -0.0010347928369753845, 'y': -2.547178344210644e-05, 'theta': 0.20078742504119873, 'speed': 3.3407051963037118, 'x_command_near': -111.37087112878058, 'y_command_near': -0.5488607890934241, 'command_near': 4, 'should_brake': False, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0426.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 3.341 m/s \\n             Your current angle is 0.201  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-111.371,-0.549) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.157, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0429.png', 'x': -0.001030545934128213, 'y': -2.400246761297022e-05, 'theta': 0.2516763508319855, 'speed': 3.289021707711032, 'x_command_near': -110.90235136977596, 'y_command_near': 0.13068008440663018, 'command_near': 4, 'should_brake': False, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0429.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 3.289 m/s \\n             Your current angle is 0.252  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-110.902,0.131) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.184, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0432.png', 'x': -0.0010263115048729787, 'y': -2.2261359357282815e-05, 'theta': 0.3013302683830261, 'speed': 3.5840150417321723, 'x_command_near': -110.90235136977596, 'y_command_near': 0.13068008440663018, 'command_near': 4, 'should_brake': False, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0432.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 3.584 m/s \\n             Your current angle is 0.301  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-110.902,0.131) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.176, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0435.png', 'x': -0.0010215268684277135, 'y': -1.9979067150844167e-05, 'theta': 0.3684180974960327, 'speed': 4.1390144513185, 'x_command_near': -110.50277398527332, 'y_command_near': 0.842455686286954, 'command_near': 4, 'should_brake': True, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0435.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 4.139 m/s \\n             Your current angle is 0.368  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-110.503,0.842) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.164, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0438.png', 'x': -0.001016393794870396, 'y': -1.6956939347916025e-05, 'theta': 0.4504754841327667, 'speed': 4.470312969974126, 'x_command_near': -110.17007894761599, 'y_command_near': 1.5878388903325302, 'command_near': 4, 'should_brake': True, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0438.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 4.47 m/s \\n             Your current angle is 0.45  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-110.17,1.588) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.129, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0441.png', 'x': -0.001011752947007949, 'y': -1.3604558966674488e-05, 'theta': 0.5295760631561279, 'speed': 3.8897540109877804, 'x_command_near': -109.90706637295887, 'y_command_near': 2.3605620893227495, 'command_near': 4, 'should_brake': False, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0441.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 3.89 m/s \\n             Your current angle is 0.53  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-109.907,2.361) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.274, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0444.png', 'x': -0.0010081742018144269, 'y': -1.0485571566619711e-05, 'theta': 0.5987879037857056, 'speed': 3.262755061391952, 'x_command_near': -109.90706637295887, 'y_command_near': 2.3605620893227495, 'command_near': 4, 'should_brake': False, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0444.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 3.263 m/s \\n             Your current angle is 0.599  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-109.907,2.361) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.036, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0447.png', 'x': -0.0010050272335178079, 'y': -7.273696445807776e-06, 'theta': 0.670066237449646, 'speed': 3.3584526964350996, 'x_command_near': -109.7159412571607, 'y_command_near': 3.1541278405264355, 'command_near': 4, 'should_brake': False, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0447.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 3.358 m/s \\n             Your current angle is 0.67  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-109.716,3.154) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.043, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0450.png', 'x': -0.0010018161840292805, 'y': -3.603577734636148e-06, 'theta': 0.7389339804649353, 'speed': 3.7561475299312184, 'x_command_near': -109.55501467344418, 'y_command_near': 4.80721951186221, 'command_near': 4, 'should_brake': False, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0450.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,-0.0) \\n             Your current speed is 3.756 m/s \\n             Your current angle is 0.739  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-109.555,4.807) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.016, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0453.png', 'x': -0.0009984747105136194, 'y': 6.940090287732837e-07, 'theta': 0.8161476254463196, 'speed': 4.188446424149144, 'x_command_near': -109.55501467344418, 'y_command_near': 4.80721951186221, 'command_near': 4, 'should_brake': True, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0453.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,0.0) \\n             Your current speed is 4.188 m/s \\n             Your current angle is 0.816  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-109.555,4.807) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.157, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0456.png', 'x': -0.0009952442653542448, 'y': 5.7140790596869204e-06, 'theta': 0.9081465005874634, 'speed': 4.432361736637046, 'x_command_near': -109.55501467344418, 'y_command_near': 4.80721951186221, 'command_near': 4, 'should_brake': True, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0456.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,0.0) \\n             Your current speed is 4.432 m/s \\n             Your current angle is 0.908  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-109.555,4.807) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is True \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : -0.042, 'brake': 1.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0459.png', 'x': -0.0009925792423217672, 'y': 1.0759181268500908e-05, 'theta': 0.9820582866668701, 'speed': 3.8962802142169584, 'x_command_near': -109.55501467344418, 'y_command_near': 4.80721951186221, 'command_near': 4, 'should_brake': False, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0459.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,0.0) \\n             Your current speed is 3.896 m/s \\n             Your current angle is 0.982  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-109.555,4.807) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.032, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0462.png', 'x': -0.0009905972492560977, 'y': 1.50835876344926e-05, 'theta': 1.038794755935669, 'speed': 3.311024301920238, 'x_command_near': -109.55139817440013, 'y_command_near': 5.807215698527921, 'command_near': 4, 'should_brake': False, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0462.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,0.0) \\n             Your current speed is 3.311 m/s \\n             Your current angle is 1.039  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-109.551,5.807) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.016, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0465.png', 'x': -0.0009889693817655143, 'y': 1.927310119733493e-05, 'theta': 1.0970956087112427, 'speed': 3.2883540286972273, 'x_command_near': -109.54778930384596, 'y_command_near': 6.807208070496361, 'command_near': 4, 'should_brake': False, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0465.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,0.0) \\n             Your current speed is 3.288 m/s \\n             Your current angle is 1.097  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-109.548,6.807) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.013, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0468.png', 'x': -0.0009876229231764455, 'y': 2.3539320484757784e-05, 'theta': 1.156588077545166, 'speed': 3.274853576408442, 'x_command_near': -109.54778930384596, 'y_command_near': 6.807208070496361, 'command_near': 4, 'should_brake': False, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0468.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,0.0) \\n             Your current speed is 3.275 m/s \\n             Your current angle is 1.157  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-109.548,6.807) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : -0.027, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0471.png', 'x': -0.0009864813872724199, 'y': 2.786572938750087e-05, 'theta': 1.2089259624481201, 'speed': 3.295262802825978, 'x_command_near': -109.54418043487381, 'y_command_near': 7.807200442464801, 'command_near': 4, 'should_brake': False, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0471.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,0.0) \\n             Your current speed is 3.295 m/s \\n             Your current angle is 1.209  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-109.544,7.807) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : 0.37, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0474.png', 'x': -0.000985558961005495, 'y': 3.2018405911994965e-05, 'theta': 1.255491852760315, 'speed': 2.9853793207056087, 'x_command_near': -109.54418043487381, 'y_command_near': 7.807200442464801, 'command_near': 4, 'should_brake': False, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0474.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,0.0) \\n             Your current speed is 2.985 m/s \\n             Your current angle is 1.255  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-109.544,7.807) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :0.0, 'steer' : 0.196, 'brake': 0.038 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0477.png', 'x': -0.0009847994448648478, 'y': 3.610243076480011e-05, 'theta': 1.2972347736358643, 'speed': 3.176835814849791, 'x_command_near': -109.54057156590166, 'y_command_near': 8.807196629130512, 'command_near': 4, 'should_brake': False, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0477.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,0.0) \\n             Your current speed is 3.177 m/s \\n             Your current angle is 1.297  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-109.541,8.807) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : 0.101, 'brake': 0.0 }\n",
      "{'rgb': 'data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0480.png', 'x': -0.0009840905768498942, 'y': 4.0665678725342786e-05, 'theta': 1.3419818878173828, 'speed': 3.6107971936488816, 'x_command_near': -109.54057156590166, 'y_command_near': 8.807196629130512, 'command_near': 4, 'should_brake': False, 'x_target': -109.45031931023617, 'y_target': 33.80703358489672, 'target_command': 4}\n",
      "Picture 1: <img>/home/ubuntu/Carla_server/Carla_Repo/TCP/data/results_LLVM/routes_lav_valid_12_18_02_14_54/input_image/0480.png</img>\\n Given this Ego centric image,             now you are autonomous driving agent, now your current position(x,y) is at             (-0.001,0.0) \\n             Your current speed is 3.611 m/s \\n             Your current angle is 1.342  \\n             Your immediate command to follow is FOLLOW THE CURRENT LANE and immediate target position is at             (-109.541,8.807) \\n             Your later target command to follow is FOLLOW THE CURRENT LANE and later target position is at             (-109.45,33.807) \\n             The current command for applying brakes is False \\n             Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\n in the json format             For example: the output can be this json { \"throttle\" :x, \"steer\" : y, \"brake\": z }\n",
      "{ 'throttle' :1.0, 'steer' : 0.118, 'brake': 0.0 }\n"
     ]
    },
    {
     "ename": "ConnectionResetError",
     "evalue": "[Errno 104] Connection reset by peer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 14\u001b[0m\n\u001b[1;32m      7\u001b[0m responder_socket \u001b[38;5;241m=\u001b[39m listen_for_initiator(host, port)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 14\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mresponder_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     SEND_DATA \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(data\u001b[38;5;241m.\u001b[39mdecode())\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(SEND_DATA)\n",
      "\u001b[0;31mConnectionResetError\u001b[0m: [Errno 104] Connection reset by peer"
     ]
    }
   ],
   "source": [
    "host = '127.0.0.1'\n",
    "port = 12345\n",
    "\n",
    "mapping = {1:\"GO LEFT\" , 2: \"GO RIGHT\" ,  3: \"GO STRAIGHT\" , 4: \"FOLLOW THE CURRENT LANE\" , 5: \"CHANGE TO LANE LEFT\" , 6: \"CHANGE TO LANE RIGHT\" }\n",
    "\n",
    "print(\"Responder listening for Initiator...\")\n",
    "responder_socket = listen_for_initiator(host, port)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "\n",
    "    data = responder_socket.recv(1024)\n",
    "\n",
    "    SEND_DATA = json.loads(data.decode())\n",
    "    print(SEND_DATA)\n",
    "\n",
    "    image_path = \"/home/ubuntu/Carla_server/Carla_Repo/TCP/\"+ SEND_DATA[\"rgb\"]\n",
    "\n",
    "    # query = f'Picture 1: <img>{image_path}</img> Given this Ego centric image, now you are autonomous driving agent, now your current position(x,y) is at ({SEND_DATA[\"x\"]},{SEND_DATA[\"y\"]}) Your current speed is {SEND_DATA[\"speed\"]} m/s Your current angle is {SEND_DATA[\"theta\"]} Your immediate command to follow is {mapping[SEND_DATA[\"command_near\"]]} and immediate target position is at ({SEND_DATA[\"x_command_near\"]},{SEND_DATA[\"y_command_near\"]}) Your later target command to follow is {mapping[SEND_DATA[\"target_command\"]]} and later target position is at ({SEND_DATA[\"x_target\"]},{SEND_DATA[\"y_target\"]}) The current command for applying brakes is {SEND_DATA[\"should_brake\"]} Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1) in the json format For example: the output can be this json {{ \"throttle\" :x, \"steer\" : y, \"brake\": z }}'\n",
    "    query = f'Picture 1: <img>{image_path}</img>\\\\n Given this Ego centric image, \\\n",
    "            now you are autonomous driving agent, now your current position(x,y) is at \\\n",
    "            ({round(SEND_DATA[\"x\"],3)},{round(SEND_DATA[\"y\"],3)}) \\\\n \\\n",
    "            Your current speed is {round(SEND_DATA[\"speed\"],3)} m/s \\\\n \\\n",
    "            Your current angle is {round(SEND_DATA[\"theta\"],3)}  \\\\n \\\n",
    "            Your immediate command to follow is {mapping[SEND_DATA[\"command_near\"]]} and immediate target position is at \\\n",
    "            ({round(SEND_DATA[\"x_command_near\"],3)},{round(SEND_DATA[\"y_command_near\"],3)}) \\\\n \\\n",
    "            Your later target command to follow is {mapping[SEND_DATA[\"target_command\"]]} and later target position is at \\\n",
    "            ({round(SEND_DATA[\"x_target\"],3)},{round(SEND_DATA[\"y_target\"],3)}) \\\\n \\\n",
    "            The current command for applying brakes is {SEND_DATA[\"should_brake\"]} \\\\n \\\n",
    "            Now you have to predict following values which are throttle (also called acceleration) (0 to 1) , steer (-1 to 1) and brake (0 to 1)\\\\n in the json format \\\n",
    "            For example: the output can be this json {{ \"throttle\" :x, \"steer\" : y, \"brake\": z }}'\n",
    "    print(query)\n",
    "    response, history = model.chat(tokenizer, query=query, history=None)\n",
    "    print(response)\n",
    "\n",
    "    response = ast.literal_eval(response)\n",
    "    response = json.dumps(response)\n",
    "\n",
    "    responder_socket.sendall(response.encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5883a6f2-cca8-46fb-b6ac-f8ff6a4b261b",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"{ 'throttle' :0.0, 'steer' : -0.005966780083557129, 'brake': 1.0 }\"\n",
    "import ast \n",
    "print(type(ast.literal_eval(name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de314142-7525-493b-94a8-9ca06db91bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
